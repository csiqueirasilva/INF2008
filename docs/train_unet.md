# UNet Training Command

The `spine train-unet` command trains a baseline UNet using the manifest generated by the pseudo-lateral pipeline.

## Expected dataset layout

Run `scripts/build_pseudo_dataset.sh` to generate angle folders. Each angle should contain:

```
angles/<rot_id>/manifest.csv        # two columns: image, mask_labels
angles/<rot_id>/images/<stem>.png
angles/<rot_id>/mask_labels/<stem>.png
...
```

The manifest is the only input required by the training command.

## Usage

```
poetry run spine train-unet \
  --csv outputs/pseudo_lateral/angles/p+90_y+0_r+0/manifest.csv \
  --out-dir runs/unet_exp01 \
  --epochs 10 \
  --batch-size 4 \
  --learning-rate 1e-3 \
  --val-fraction 0.2
```

### Options

| Flag | Description |
|------|-------------|
| `--csv` | Path to a manifest CSV (must contain `image` and `mask_labels`). |
| `--out-dir` | Directory in which checkpoints and logs will be stored. |
| `--epochs` | Number of epochs (default: 5). |
| `--batch-size` | Mini-batch size (default: 4). |
| `--learning-rate` | Adam optimiser learning rate (default: 1e-3). |
| `--val-fraction` | Fraction of the dataset reserved for validation (default: 0.1). Set to 0 to disable validation. |
| `--num-workers` | Number of DataLoader workers (default: 2). |
| `--device` | Training device (defaults to CUDA if available). |
| `--seed` | RNG seed applied to NumPy, PyTorch, and Python `random`. |

## Outputs

After training, `out-dir` will contain:

- `unet_best.pt` – checkpoint with model weights, number of classes, and config snapshot.
- `training_summary.json` – metadata dump with hyper-parameters and per-epoch losses.

If no validation set is used, the final epoch model is saved as the “best” checkpoint.

## Architecture & loss

- Simple UNet encoder–decoder with channels `[32, 64, 128, 256, 256]`.
- Input is a single-channel image (normalised to [0, 1]).
- Output logits are trained using `CrossEntropyLoss` against the multi-class mask. `SpineDataset` automatically infers the number of classes from `mask_labels`.

## Extending the training script

- Augmentations, multi-GPU training, and better logging can be added later.
- The manifest format is deliberately minimal so new dataloaders can be swapped in without touching the generator pipeline.
